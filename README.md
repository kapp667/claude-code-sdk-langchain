# Claude Code SDK - LangChain Adapter

Utilisez Claude via votre abonnement Claude Code (20$/mois) comme mod√®le LLM dans LangChain pour prototyper des applications agentiques **SANS frais API suppl√©mentaires** !

## üéØ Objectif

Cet adaptateur permet d'utiliser votre abonnement Claude Code existant comme backend pour LangChain, vous permettant de :
- ‚úÖ Prototyper des applications LangChain gratuitement (via votre abonnement)
- ‚úÖ Tester des id√©es d'agents sans se soucier des co√ªts API
- ‚úÖ Migrer facilement vers l'API officielle en production

## üì¶ Installation

```bash
# 1. Installer les d√©pendances
pip install claude-code-sdk langchain langchain-core

# 2. S'assurer que Claude Code CLI est install√©
npm install -g @anthropic-ai/claude-code
```

## üöÄ Utilisation Rapide

```python
from src.claude_code_langchain import ClaudeCodeChatModel
from langchain_core.prompts import ChatPromptTemplate

# Cr√©er le mod√®le (utilise votre abonnement Claude Code)
model = ClaudeCodeChatModel(
    model="claude-sonnet-4-20250514",
    temperature=0.7
)

# Utilisation simple
response = model.invoke("Qu'est-ce que LangChain?")
print(response.content)

# Dans une cha√Æne LangChain
prompt = ChatPromptTemplate.from_messages([
    ("system", "Tu es un assistant expert en {domain}"),
    ("human", "{question}")
])

chain = prompt | model
result = chain.invoke({
    "domain": "Python",
    "question": "Comment cr√©er une API REST?"
})
```

## üîÑ Streaming

```python
# Streaming de r√©ponses
for chunk in model.stream("Raconte une histoire"):
    print(chunk.content, end="")

# Streaming asynchrone
async for chunk in model.astream("Liste 5 id√©es"):
    print(chunk.content, end="")
```

## üîó Int√©gration LangChain Compl√®te

L'adaptateur supporte toutes les fonctionnalit√©s LangChain :
- ‚úÖ Invocation synchrone/asynchrone
- ‚úÖ Streaming
- ‚úÖ Batch processing
- ‚úÖ Int√©gration LCEL (LangChain Expression Language)
- ‚úÖ Cha√Ænes et agents

## üìù Exemples

Voir le dossier `examples/` pour des exemples complets :
- `basic_usage.py` - Exemples d'utilisation vari√©s
- Tests dans `specs/` - Tests de flux pragmatiques

## üß™ Tests

```bash
# Ex√©cuter les tests de flux
python specs/flow_basic_chat_test.py
python specs/flow_langchain_integration_test.py

# Ou avec pytest
pytest specs/
```

## üîÑ Migration vers Production

Quand vous √™tes pr√™t pour la production, remplacez simplement :

```python
# D√©veloppement (votre abonnement)
from src.claude_code_langchain import ClaudeCodeChatModel
model = ClaudeCodeChatModel(model="claude-sonnet-4-20250514")

# Production (API officielle)
from langchain_anthropic import ChatAnthropic
model = ChatAnthropic(model="claude-3-opus-20240229", api_key="sk-...")
```

Le reste de votre code reste identique !

## ‚öôÔ∏è Configuration

```python
model = ClaudeCodeChatModel(
    model="claude-sonnet-4-20250514",     # Mod√®le Claude Sonnet 4
    temperature=0.7,                      # ‚ö†Ô∏è NON SUPPORT√â (valeur ignor√©e)
    max_tokens=2000,                      # ‚ö†Ô∏è NON SUPPORT√â (valeur ignor√©e)
    system_prompt="Tu es un expert...",  # Prompt syst√®me
    permission_mode="default",            # Mode permissions Claude Code
)
```

## üèóÔ∏è Architecture

```
LangChain App
     ‚Üì
ClaudeCodeChatModel (cet adaptateur)
     ‚Üì
claude-code-sdk (SDK Python)
     ‚Üì
Claude Code CLI
     ‚Üì
Claude (via votre abonnement)
```

## ‚ö†Ô∏è Limitations

### Temperature et Max_Tokens

Le Claude Code CLI ne supporte pas les param√®tres `temperature` et `max_tokens`. Ces param√®tres sont accept√©s pour compatibilit√© API mais **n'ont aucun effet**.

**Pour le D√©veloppement (Claude Code):**
```python
model = ClaudeCodeChatModel()  # Utilise les valeurs par d√©faut du mod√®le
# temperature=0.7 et max_tokens=2000 n'auront aucun effet
```

**Pour la Production (avec contr√¥le des param√®tres):**
```python
model = ChatAnthropic(
    temperature=0.7,
    max_tokens=1000,
    api_key=os.getenv("ANTHROPIC_API_KEY")
)
```

Si vous avez besoin du contr√¥le de temp√©rature ou de limite de tokens pendant le d√©veloppement, utilisez l'API de production directement avec votre cl√© API Anthropic.

### Support Async

L'adaptateur supporte les op√©rations asynchrones avec certaines limitations :

**‚úÖ Op√©rations Sync (Support Complet - 100%)**
- `model.invoke()` - Support complet
- `model.stream()` - Support complet
- `model.batch()` - Support complet
- Cha√Ænes avec ex√©cution sync - Support complet

**‚ö†Ô∏è Op√©rations Async (Support Partiel)**
- ‚úÖ `model.ainvoke()` - Support complet
- ‚úÖ `model.astream()` - Streaming basique support√©
- ‚ö†Ô∏è `chain.astream()` avec parsers - **Support limit√©** (probl√®me anyio/asyncio)
- ‚ùå Cancellation de stream - **Non support√© actuellement**

**Recommandation** :
- Pour prototypage avec notebooks, scripts, cha√Ænes basiques ‚Üí **Utiliser m√©thodes sync**
- Pour besoins async avanc√©s (parsers, cancellation) ‚Üí **Utiliser API production (ChatAnthropic)**
- 70% des cas d'usage prototypage = sync suffit amplement

**Tests** : 14/16 tests passent (87.5%) - les √©checs concernent async avanc√© uniquement.

### Autres Limitations

- Pas de support natif des tool calls (peut √™tre ajout√© via prompting)
- Pas de support vision/multimodal (images d√©tect√©es et warning √©mis)
- Latence potentiellement plus √©lev√©e que l'API directe (subprocess overhead)
- N√©cessite Claude Code CLI install√© localement
- Limit√© par les quotas de votre abonnement Claude Code

## ü§ù Contribution

Les contributions sont bienvenues ! N'h√©sitez pas √† :
- Ajouter des fonctionnalit√©s
- Am√©liorer la documentation
- Rapporter des bugs
- Proposer des optimisations

## üìÑ Licence

MIT

## üôè Remerciements

- Anthropic pour Claude et Claude Code
- LangChain pour le framework
- St√©phane Wootha Richard pour l'orchestration

---

**Note** : Cet adaptateur est parfait pour le prototypage et le d√©veloppement. Pour la production √† grande √©chelle, consid√©rez l'API officielle Anthropic.