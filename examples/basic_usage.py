"""
Exemple d'utilisation de ClaudeCodeChatModel avec LangChain

Ce script montre comment utiliser Claude via votre abonnement Claude Code
pour prototyper des applications LangChain SANS frais API suppl√©mentaires.
"""

import asyncio
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

# Import de notre adaptateur
from claude_code_langchain import ClaudeCodeChatModel


def example_simple_invocation():
    """Exemple 1: Invocation simple du mod√®le"""
    print("\nüìù Exemple 1: Invocation Simple")
    print("-" * 40)

    # Cr√©er le mod√®le (utilise votre abonnement Claude Code)
    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.7,
        max_tokens=500
    )

    # Envoyer un message simple
    response = model.invoke([
        HumanMessage(content="Qu'est-ce que LangChain en 2 phrases?")
    ])

    print(f"R√©ponse: {response.content}")


def example_with_system_prompt():
    """Exemple 2: Utilisation avec prompt syst√®me"""
    print("\nüéØ Exemple 2: Avec Prompt Syst√®me")
    print("-" * 40)

    # Mod√®le avec configuration syst√®me
    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        system_prompt="Tu es un expert Python qui r√©pond de mani√®re tr√®s concise.",
        temperature=0.3,
        max_tokens=200
    )

    # Messages avec contexte syst√®me
    messages = [
        SystemMessage(content="Utilise des exemples de code quand c'est pertinent"),
        HumanMessage(content="Comment cr√©er une liste en Python?")
    ]

    response = model.invoke(messages)
    print(f"R√©ponse: {response.content}")


def example_streaming():
    """Exemple 3: Streaming de r√©ponses"""
    print("\nüåä Exemple 3: Streaming")
    print("-" * 40)

    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.8
    )

    print("Streaming de la r√©ponse: ", end="")
    for chunk in model.stream([
        HumanMessage(content="Raconte une tr√®s courte histoire sur un robot")
    ]):
        print(chunk.content, end="", flush=True)
    print()  # Nouvelle ligne √† la fin


def example_langchain_chain():
    """Exemple 4: Cha√Æne LangChain compl√®te"""
    print("\nüîó Exemple 4: Cha√Æne LangChain")
    print("-" * 40)

    # Cr√©er les composants de la cha√Æne
    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.5
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", "Tu es un assistant expert en {domain}"),
        ("human", "{question}")
    ])

    # Construire la cha√Æne avec LCEL
    chain = prompt | model | StrOutputParser()

    # Invoquer la cha√Æne
    response = chain.invoke({
        "domain": "intelligence artificielle",
        "question": "Qu'est-ce qu'un r√©seau de neurones?"
    })

    print(f"R√©ponse: {response}")


async def example_async_operations():
    """Exemple 5: Op√©rations asynchrones"""
    print("\n‚ö° Exemple 5: Op√©rations Asynchrones")
    print("-" * 40)

    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.6
    )

    # Invocation asynchrone
    response = await model.ainvoke([
        HumanMessage(content="Qu'est-ce que Python async/await?")
    ])

    print(f"R√©ponse async: {response.content[:200]}...")

    # Streaming asynchrone
    print("\nStreaming async: ", end="")
    async for chunk in model.astream([
        HumanMessage(content="Liste 3 avantages de l'async")
    ]):
        print(".", end="", flush=True)
    print(" Termin√©!")


def example_batch_processing():
    """Exemple 6: Traitement par batch"""
    print("\nüì¶ Exemple 6: Traitement Batch")
    print("-" * 40)

    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.4,
        max_tokens=100
    )

    # Plusieurs questions en batch
    questions = [
        [HumanMessage(content="Capitale de la France?")],
        [HumanMessage(content="Capitale de l'Espagne?")],
        [HumanMessage(content="Capitale de l'Italie?")]
    ]

    responses = model.batch(questions)

    for i, response in enumerate(responses, 1):
        print(f"Question {i}: {response.content}")


def example_complex_agent_chain():
    """Exemple 7: Cha√Æne d'agent complexe"""
    print("\nü§ñ Exemple 7: Cha√Æne Complexe (Agent-like)")
    print("-" * 40)

    from langchain_core.runnables import RunnablePassthrough

    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.5
    )

    # Premi√®re √©tape: Analyser l'intention
    intent_prompt = ChatPromptTemplate.from_messages([
        ("system", "D√©termine si la question est technique ou g√©n√©rale"),
        ("human", "{question}")
    ])

    # Deuxi√®me √©tape: R√©pondre selon l'intention
    response_prompt = ChatPromptTemplate.from_messages([
        ("system", "R√©ponds √† cette question {intent}"),
        ("human", "{question}")
    ])

    # Cha√Æne d'analyse d'intention
    intent_chain = intent_prompt | model | StrOutputParser()

    # Cha√Æne compl√®te avec routing
    def route_response(inputs):
        intent = intent_chain.invoke({"question": inputs["question"]})
        return {
            "intent": "de mani√®re technique" if "technique" in intent.lower() else "simplement",
            "question": inputs["question"]
        }

    full_chain = (
        RunnablePassthrough()
        | route_response
        | response_prompt
        | model
        | StrOutputParser()
    )

    response = full_chain.invoke({
        "question": "Comment fonctionne une API REST?"
    })

    print(f"R√©ponse adapt√©e: {response[:200]}...")


def example_continuous_session():
    """Exemple 8: Conversation multi-tour (gestion manuelle du contexte)"""
    print("\nüí≠ Exemple 8: Conversation Multi-Tour")
    print("-" * 40)

    # ClaudeCodeChatModel est STATELESS par design (compatible LangChain)
    # Pour maintenir le contexte, passer explicitement l'historique
    model = ClaudeCodeChatModel(
        model="claude-sonnet-4-20250514",
        temperature=0.6
    )

    print("Note: Le mod√®le est stateless (recommand√© pour LangChain)")
    print("Le contexte est g√©r√© en passant l'historique explicitement")

    # Construire l'historique manuellement
    conversation = []

    # Premi√®re interaction
    conversation.append(HumanMessage(content="Je m'appelle Alice"))
    response1 = model.invoke(conversation)
    print(f"R1: {response1.content}")

    # Ajouter la r√©ponse √† l'historique
    conversation.append(response1)

    # Deuxi√®me interaction avec contexte
    conversation.append(HumanMessage(content="Quel est mon nom?"))
    response2 = model.invoke(conversation)
    print(f"R2: {response2.content}")
    print("\nContexte maintenu via historique explicite ‚úì")


def main():
    """Fonction principale pour ex√©cuter tous les exemples"""
    print("=" * 50)
    print("üöÄ Exemples ClaudeCodeChatModel pour LangChain")
    print("=" * 50)
    print("\nUtilise votre abonnement Claude Code (20$/mois)")
    print("AUCUN frais API suppl√©mentaire!")

    # Ex√©cuter les exemples synchrones
    example_simple_invocation()
    example_with_system_prompt()
    example_streaming()
    example_langchain_chain()
    example_batch_processing()
    example_complex_agent_chain()

    # Ex√©cuter les exemples asynchrones
    print("\n" + "=" * 50)
    print("Exemples Asynchrones")
    print("=" * 50)
    asyncio.run(example_async_operations())

    # Session continue (optionnel)
    example_continuous_session()

    print("\n" + "=" * 50)
    print("‚úÖ Tous les exemples ex√©cut√©s avec succ√®s!")
    print("=" * 50)
    print("\nüí° Astuce: Remplacez ClaudeCodeChatModel par ChatAnthropic")
    print("   quand vous serez pr√™t pour la production avec l'API officielle!")


if __name__ == "__main__":
    main()